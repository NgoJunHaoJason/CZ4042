{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38364bitaivenvcondaf74d87cbcd1c41a2a9ab01eaafaa18b6",
   "display_name": "Python 3.8.3 64-bit ('ai_venv': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Evaluate MobileNet V2"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Import relevant libraries"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "import logging\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import wandb"
   ]
  },
  {
   "source": [
    "wandb.login()"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mburntice\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 34661<br/>Program ended successfully."
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find user logs for this run at: <code>wandb/run-20201030_153820-vnlmg1xa/logs/debug.log</code>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find internal logs for this run at: <code>wandb/run-20201030_153820-vnlmg1xa/logs/debug-internal.log</code>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    <br/>Synced <strong style=\"color:#cdcd00\">resnet50_v2</strong>: <a href=\"https://wandb.ai/burntice/cz4042/runs/vnlmg1xa\" target=\"_blank\">https://wandb.ai/burntice/cz4042/runs/vnlmg1xa</a><br/>\n                "
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                Tracking run with wandb version 0.10.8<br/>\n                Syncing run <strong style=\"color:#cdcd00\">resnet50_v2</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n                Project page: <a href=\"https://wandb.ai/burntice/cz4042\" target=\"_blank\">https://wandb.ai/burntice/cz4042</a><br/>\n                Run page: <a href=\"https://wandb.ai/burntice/cz4042/runs/3db96pa6\" target=\"_blank\">https://wandb.ai/burntice/cz4042/runs/3db96pa6</a><br/>\n                Run data is saved locally in <code>wandb/run-20201030_153833-3db96pa6</code><br/><br/>\n            "
     },
     "metadata": {}
    }
   ],
   "source": [
    "# Initialise Weights and Biases API\n",
    "# Hyperparameters\n",
    "run = wandb.init(\n",
    "    name='resnet50_v2',\n",
    "    project='cz4042',\n",
    "    config={\n",
    "        'batch_size': 128,\n",
    "        'epochs': 20,\n",
    "        'seed': 0,\n",
    "        'optimizer': 'sgd',\n",
    "        'loss_function': 'binary_crossentropy',\n",
    "        'metrics': ['accuracy'],\n",
    "    },\n",
    ")\n",
    "\n",
    "config = wandb.config\n",
    "\n",
    "logger = logging.getLogger('wandb')\n",
    "logger.setLevel(logging.ERROR)"
   ]
  },
  {
   "source": [
    "Prevent GPU memory usage from exceeding GPU memory"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "# reference: https://github.com/tensorflow/tensorflow/issues/25160#issuecomment-643703167\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    # Restrict TensorFlow to only allocate 1GB * 2 of memory on the first GPU\n",
    "    try:\n",
    "        tf.config.experimental.set_virtual_device_configuration(\n",
    "            gpus[0],\n",
    "            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024 * 4)],\n",
    "        )\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), 'Physical GPUs,', len(logical_gpus), 'Logical GPUs')\n",
    "    except RuntimeError as error:\n",
    "        # Virtual devices must be set before GPUs have been initialized\n",
    "        print(error)\n",
    "else:\n",
    "    print('No GPU detected!')"
   ]
  },
  {
   "source": [
    "Load data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file_path = '/home/burntice/0_repositories/CZ4042/project/data/aligned_gender.csv'\n",
    "data_dir_path = '/home/burntice/3_data/Adience/'\n",
    "\n",
    "dataframe = pd.read_csv(data_file_path, sep='\\t', names=['datadir', 'gender'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                             datadir  gender\n",
       "0                                     datadir gender     NaN\n",
       "1  aligned/30601258@N03/landmark_aligned_face.2.1...     1.0\n",
       "2  aligned/30601258@N03/landmark_aligned_face.3.1...     1.0\n",
       "3  aligned/30601258@N03/landmark_aligned_face.2.1...     1.0\n",
       "4  aligned/30601258@N03/landmark_aligned_face.4.1...     0.0"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>datadir</th>\n      <th>gender</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>datadir gender</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>aligned/30601258@N03/landmark_aligned_face.2.1...</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>aligned/30601258@N03/landmark_aligned_face.3.1...</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>aligned/30601258@N03/landmark_aligned_face.2.1...</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>aligned/30601258@N03/landmark_aligned_face.4.1...</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "# confirm dataframe is ok\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(12195, 2)"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "# check number of data in dataframe\n",
    "dataframe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.0    52.660927\n",
       "1.0    47.330873\n",
       "Name: gender, dtype: float64"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "# check percentage of gender in data\n",
    "dataframe['gender'].value_counts() / dataframe.shape[0] * 100"
   ]
  },
  {
   "source": [
    "Not very imbalanced; good."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    dataframe['datadir'],\n",
    "    dataframe['gender'],\n",
    "    test_size=0.2,\n",
    "    random_state=config.seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "6753     aligned/7636528@N03/landmark_aligned_face.1052...\n",
       "8876     aligned/113650443@N02/landmark_aligned_face.14...\n",
       "9269     aligned/48647239@N03/landmark_aligned_face.151...\n",
       "1022     aligned/37303189@N08/landmark_aligned_face.84....\n",
       "10913    aligned/10280355@N07/landmark_aligned_face.189...\n",
       "Name: datadir, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "# checking to make sure everything is alright\n",
    "x_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.concat([x_train, y_train], axis=1)\n",
    "test_df = pd.concat([x_test, y_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(9756, 2)"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.0    52.97253\n",
       "1.0    47.01722\n",
       "Name: gender, dtype: float64"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "train_df['gender'].value_counts() / train_df.shape[0] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found 9755 validated image filenames.\nFound 2439 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "# Load images into keras image generator \n",
    "datagen_train = ImageDataGenerator(rescale=1./255)\n",
    "datagen_val = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# For train generator\n",
    "train_generator = datagen_train.flow_from_dataframe(\n",
    "    dataframe=train_df,\n",
    "    directory=data_dir_path,\n",
    "    x_col='datadir',\n",
    "    y_col='gender',\n",
    "    batch_size=config.batch_size,\n",
    "    seed=config.seed,\n",
    "    shuffle=True,\n",
    "    class_mode='raw',\n",
    "    target_size=(224,224),\n",
    ")\n",
    "\n",
    "# For test generator \n",
    "val_generator = datagen_val.flow_from_dataframe(\n",
    "    dataframe = test_df,\n",
    "    directory=data_dir_path,\n",
    "    x_col='datadir',\n",
    "    y_col='gender',\n",
    "    batch_size=config.batch_size,\n",
    "    seed=config.seed,\n",
    "    shuffle=True,\n",
    "    class_mode='raw',\n",
    "    target_size=(224,224),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
      "9412608/9406464 [==============================] - 0s 0us/step\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "mobilenetv2_1.00_224 (Model) (None, 1280)              2257984   \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1280)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 1280)              5120      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1024)              1311744   \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 4,238,401\n",
      "Trainable params: 1,974,273\n",
      "Non-trainable params: 2,264,128\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.applications.MobileNetV2(include_top=False, pooling='avg', weights='imagenet'),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(1024, activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(1, activation='softmax'),\n",
    "])\n",
    "\n",
    "model.layers[0].trainable = False\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model \n",
    "model.compile(\n",
    "    optimizer=config.optimizer,\n",
    "    loss=config.loss_function,\n",
    "    metrics=config.metrics,\n",
    ")"
   ]
  },
  {
   "source": [
    "Define callback for saving weights while training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir_path = '/home/burntice/0_repositories/CZ4042/project/checkpoints/'\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=(checkpoint_dir_path + 'cp-{epoch:02d}-{val_loss:.2f}.hdf5'),\n",
    "    save_weights_only=True,\n",
    "    save_best_only=True,\n",
    "    monitor='val_loss'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/20\n",
      "  12353/Unknown - 14368s 1s/step - loss: 8.0784 - accuracy: 0.4702"
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_generator,\n",
    "    epochs=config.epochs,\n",
    "    validation_data=val_generator,\n",
    "    callbacks=[WandbCallback(), cp_callback],\n",
    ")"
   ]
  },
  {
   "source": [
    "Let Weights and Biases know that this run is complete."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.finish()"
   ]
  }
 ]
}